{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddffd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def zdt1(x):\n",
    "    n = len(x)\n",
    "    f1 = x[0]\n",
    "    g = 1 + 9 * np.sum(x[1:]) / (n - 1)\n",
    "    f2 = g * (1 - np.sqrt(x[0] / g))\n",
    "    return f1, f2\n",
    "\n",
    "def zdt2(x):\n",
    "    n = len(x)\n",
    "    f1 = x[0]\n",
    "    g = 1 + 9 * np.sum(x[1:]) / (n - 1)\n",
    "    f2 = g * (1 - (x[0] / g)**2)\n",
    "    return f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7658126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dtype': torch.float64, 'device': device(type='cpu')}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numexpr\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "print(tkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4f24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini_constraints = ['x1 + x2 + x3 - ']\n",
    "\n",
    "# def check_constraints(x):\n",
    "#     return (x.sum(dim=-1) - 200).view(-1,1)\n",
    "\n",
    "# def generate_initial_data(n, bounds):\n",
    "#     # generate training data\n",
    "#     train_x = draw_sobol_samples(bounds=bounds, n=n, q=1).squeeze(1)\n",
    "#     train_con = check_constraints(train_x)\n",
    "    \n",
    "#     return train_x, train_con\n",
    "\n",
    "def initialize_model(train_x, train_obj, bounds):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        train_y = train_obj[..., i:i+1]\n",
    "        # train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        models.append(\n",
    "            SingleTaskGP(train_x, train_y, outcome_transform=Standardize(m=1)) \n",
    "        )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model\n",
    "\n",
    "\n",
    "def optimize_qnehvi(model, bounds, train_x, train_obj, sampler, ref_point, inequality_constraints=None):\n",
    "    \"\"\"Optimizes the qNEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    standard_bounds = torch.zeros(2, train_x.shape[-1], **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "    train_x = normalize(train_x, bounds)\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point.tolist(),  # use known reference point\n",
    "        X_baseline=train_x,\n",
    "        sampler=sampler,\n",
    "        prune_baseline=True,\n",
    "        # define an objective that specifies which outcomes are the objectives\n",
    "        objective=IdentityMCMultiOutputObjective(outcomes=[i for i in range(train_obj.shape[-1])]),\n",
    "        # specify that the constraint is on the last outcome\n",
    "        # constraints=[lambda Z: Z[..., -1]],\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 400},\n",
    "        sequential=True,\n",
    "        inequality_constraints=inequality_constraints\n",
    "    )\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def pre_by_model(models, x, bounds):\n",
    "    test_x = normalize(x, bounds)\n",
    "    results = None\n",
    "    for i in range(len(models)):\n",
    "        a = models[i].posterior(test_x).mean.detach().cpu().numpy()\n",
    "        b = models[i].posterior(test_x).variance.detach().cpu().numpy()\n",
    "        b=np.sqrt(b)\n",
    "        c = np.concatenate([a,b], axis=-1)\n",
    "        if results is None:\n",
    "            results = c\n",
    "        else:\n",
    "            results = np.concatenate([results, c], axis=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43cee378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_df(new_x, model_qnehvi, bounds):\n",
    "    result = pre_by_model(model_qnehvi.models, new_x, bounds)\n",
    "    new_x = new_x.detach().cpu().numpy()\n",
    "    select_df1 = pd.DataFrame({'ratio':new_x[:,0],\n",
    "                               'T1': new_x[:,1],\n",
    "                               't1': new_x[:,2],\n",
    "                               'T2': new_x[:,3],\n",
    "                               't2': new_x[:,4],\n",
    "                               'T3': new_x[:,5],\n",
    "                               't3': new_x[:,6],\n",
    "                               'T4': new_x[:,7],\n",
    "                               't4': new_x[:,8],})\n",
    "\n",
    "    select_df1[['pre_strength', 'pre_module', 'pre_elongation']] =  result[:,0].reshape(-1, BATCH_SIZE).T\n",
    "    select_df1[['pre_strength_bar', 'pre_module_bar', 'pre_elongation_bar']] =  result[:,1].reshape(-1, BATCH_SIZE).T\n",
    "    \n",
    "    select_df1 = select_df1[[\n",
    "        'ratio', 'T1', 't1', 'T2', 't2', 'T3', 't3', 'T4', 't4',\n",
    "        'pre_strength', 'pre_strength_bar', 'pre_module','pre_module_bar',\n",
    "        'pre_elongation', 'pre_elongation_bar'        \n",
    "    ]]\n",
    "    return select_df1\n",
    "\n",
    "def get_ehvi(model_qnehvi, ref_point, train_x, train_obj, can):\n",
    "    qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model_qnehvi,\n",
    "            ref_point= ref_point, # current_state['ref_point'].tolist(),  # use known reference point\n",
    "            X_baseline=train_x,\n",
    "            sampler=qnehvi_sampler,\n",
    "            prune_baseline=True,\n",
    "            # define an objective that specifies which outcomes are the objectives\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=[i for i in range(train_obj.shape[-1])]),\n",
    "            # specify that the constraint is on the last outcome\n",
    "            # constraints=[lambda Z: Z[..., -1]],\n",
    "        )\n",
    "    result = [acq_func(can[i].view(1,-1)).detach().item() for i in range(can.shape[0])]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bf4791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=''\n",
    "# from benchmark_functions import branin,Currin\n",
    "# functions=[branin,Currin]\n",
    "\n",
    "# 输入维度\n",
    "x_dim = 9\n",
    "\n",
    "# 目标\n",
    "inputs = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
    "functions = ['zdt2_y1', 'zdt2_y2']\n",
    "\n",
    "# 随机数确定\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# X的边界\n",
    "bound=[0,1]\n",
    "Fun_bounds = [bound]*x_dim\n",
    "\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [1., 1., 1., 1., 1., 1., 1., 1., 1.]] , **tkwargs)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 10 \n",
    "RAW_SAMPLES = 2048 \n",
    "N_BATCH = 20 \n",
    "MC_SAMPLES = 2048 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5001d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt(filename, batch_size, log, output):\n",
    "    df = pd.read_csv(path + filename + '.csv')\n",
    "    df = df.sample(frac=1.0)\n",
    "    train_x = np.array(df[inputs])\n",
    "    train_x = torch.tensor(train_x, **tkwargs)\n",
    "    train_obj = torch.tensor(np.array(df[functions]), **tkwargs)\n",
    "    mll_qnehvi, model_qnehvi = initialize_model(train_x, train_obj, bounds)\n",
    "    fit_gpytorch_mll(mll_qnehvi)\n",
    "    \n",
    "    ref_point = train_obj.mean(axis=0)\n",
    "    # ref_point = ref_point + train_obj.std(axis=0) * torch.tensor([0, 3.14, -1], **tkwargs)\n",
    "    hv = Hypervolume(ref_point=ref_point)\n",
    "    if train_obj.shape[0] > 0:\n",
    "        pareto_mask = is_non_dominated(train_obj)\n",
    "        pareto_y = train_obj[pareto_mask]\n",
    "        # compute hypervolume\n",
    "        volume = hv.compute(pareto_y)\n",
    "    else:\n",
    "        volume = 0.0\n",
    "\n",
    "    hvs = []\n",
    "    hvs.append(volume)\n",
    "    print(hvs)\n",
    "    \n",
    "    qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "    candidates = optimize_qnehvi(model_qnehvi, bounds, train_x, train_obj, qnehvi_sampler, ref_point)\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=bounds)\n",
    "    \n",
    "    # new_df = get_new_df(new_x, model_qnehvi, bounds)\n",
    "    # ehvi = get_ehvi(model_qnehvi, ref_point.tolist(), train_x, train_obj, candidates)\n",
    "    # new_df['ehvi'] = [ehvi[i] for i in range(len(ehvi))]\n",
    "\n",
    "    # map_bounds = np.array([[95., 120., 130., 120., 170., 120.|],\n",
    "    #                        [125., 300., 160., 300., 190., 300.]])\n",
    "\n",
    "    # tmp_x = np.array(new_df[['Temperature1/C', 'Time1/min', 'Temperature2/C','Time2/min', 'Temperature3/C', 'Time3/min']])\n",
    "    # tmp_x = (train_x-map_bounds[0,:])/(map_bounds[1,:]-map_bounds[0,:])\n",
    "\n",
    "    current_state = {'x_dim': x_dim, 'bounds': bounds, 'ref_point': ref_point, \n",
    "                      'train_x': train_x, 'train_obj': train_obj, 'hvs': hvs, 'candidates': candidates,\n",
    "                      'new_x': new_x, 'model_state_dict': model_qnehvi.state_dict(), 'df': df} #,'result_df': new_df}\n",
    "    \n",
    "    select_df = pd.DataFrame()\n",
    "    select_df[inputs] = new_x\n",
    "    select_df.to_csv('%s.csv'%output, index=False)\n",
    "    with open('%s.pkl'%log, 'wb') as f:\n",
    "        pickle.dump(current_state, f)\n",
    "\n",
    "def converge(filename, output, result):\n",
    "    df1 = pd.read_csv(output+'.csv')\n",
    "    x_ = df1[inputs]\n",
    "    x_inputs = np.array(x_)\n",
    "    zdt2_values = -np.array([zdt2(x) for x in x_inputs])\n",
    "\n",
    "    df = pd.read_csv(path + filename + '.csv')\n",
    "    x = np.array(df[inputs])\n",
    "    y = np.array(df[functions])\n",
    "    y_ = zdt2_values\n",
    "\n",
    "    new_x = np.concatenate([x, x_])\n",
    "    new_y = np.concatenate([y, y_])\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[inputs] = new_x\n",
    "    new_df[functions] = new_y\n",
    "    new_df.to_csv('%s.csv'%result, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc8f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6187559842475041]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "filename = 'test_zdt12'\n",
    "batch_size = 4\n",
    "log = 'log_%s'%i\n",
    "output = 'select_%s'%i\n",
    "opt(filename, batch_size, log, output)\n",
    "converge(filename, output, result='test_zdt12_%s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a2fc508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5874053006847657]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "filename = 'test_zdt12_%s'%(i-1)\n",
    "batch_size = 4\n",
    "log = 'log_%s'%i\n",
    "output = 'select_%s'%i\n",
    "opt(filename, batch_size, log, output)\n",
    "converge(filename, output, result='test_zdt12_%s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92129ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5953386837366733]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "filename = 'test_zdt12_%s'%(i-1)\n",
    "batch_size = 4\n",
    "log = 'log_%s'%i\n",
    "output = 'select_%s'%i\n",
    "opt(filename, batch_size, log, output)\n",
    "converge(filename, output, result='test_zdt12_%s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8845c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6601301131807672]\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "filename = 'test_zdt12_%s'%(i-1)\n",
    "batch_size = 4\n",
    "log = 'log_%s'%i\n",
    "output = 'select_%s'%i\n",
    "opt(filename, batch_size, log, output)\n",
    "converge(filename, output, result='test_zdt12_%s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96ebb424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7083437905125926]\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "filename = 'test_zdt12_%s'%(i-1)\n",
    "batch_size = 4\n",
    "log = 'log_%s'%i\n",
    "output = 'select_%s'%i\n",
    "opt(filename, batch_size, log, output)\n",
    "converge(filename, output, result='test_zdt12_%s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55ef9e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7075984920623659]\n"
     ]
    }
   ],
   "source": [
    "i = 6\n",
    "filename = 'test_zdt12_%s'%(i-1)\n",
    "batch_size = 4\n",
    "log = 'log_%s'%i\n",
    "output = 'select_%s'%i\n",
    "opt(filename, batch_size, log, output)\n",
    "converge(filename, output, result='test_zdt12_%s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177540e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
